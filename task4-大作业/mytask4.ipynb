{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task4\n",
    "\n",
    "#### 1 大作业：爬取腾讯新闻\n",
    "\n",
    "1. 了解ajax加载\n",
    "2. 通过chrome的开发者工具，监控网络请求，并分析\n",
    "3. 用selenium完成爬虫\n",
    "\n",
    "<br>用selenium爬取https://news.qq.com/ 的热点精选\n",
    "热点精选至少爬50个出来，存储成csv\n",
    "\n",
    "#### 2 进阶加餐-知乎爬虫\n",
    "链接如下\n",
    "<br>https://www.zhihu.com/search?q=Datawhale&utm_content=search_history&type=content\n",
    "用requests库实现，不能用selenium网页自动化，不仅要爬出第一页的内容，需要爬出所有的内容，给的链接仅仅是第一页的内容\n",
    "\n",
    "提示：\n",
    "- 该链接需要登录，可通过github等，搜索知乎登录的代码实现，并理解其中的逻辑，此任务允许复制粘贴代码\n",
    "- 与上面ajax加载类似，这次的ajax加载需要用requests完成爬取，最终存储样式随意，但是通过Chrome的开发者工具，分析出ajax的流程需要写出来"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from  selenium import webdriver\n",
    "driver=webdriver.Chrome(executable_path=\"E:\\Program Files\\Anaconda3\\chromedriver.exe\")\n",
    "driver.get(\"https://news.qq.com\")\n",
    "# 有个time等待两秒的过程\n",
    "for i in range(1,100):\n",
    "    time.sleep(2)\n",
    "    driver.execute_script(\"window.scrollTo(window.scrollX, %d);\"%(i*200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "html=driver.page_source\n",
    "bsObj=BeautifulSoup(html,\"lxml\")\n",
    "\n",
    "jxtits=bsObj.find_all(\"div\",{\"class\":\"jx-tit\"})[0].find_next_sibling().find_all(\"li\")\n",
    "\n",
    "print(\"index\",\",\",\"title\",\",\",\"url\")\n",
    "for i,jxtit in enumerate(jxtits):\n",
    "#     print(jxtit)\n",
    "    \n",
    "    try:\n",
    "        text=jxtit.find_all(\"img\")[0][\"alt\"]\n",
    "    except:\n",
    "        text=jxtit.find_all(\"div\",{\"class\":\"lazyload-placeholder\"})[0].text\n",
    "    try:\n",
    "        url=jxtit.find_all(\"a\")[0][\"href\"]\n",
    "    except:\n",
    "        print(jxtit)\n",
    "    print(i+1,\",\",text,\",\",url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#导出\n",
    "import pandas as pd\n",
    "name = ['index', 'title', 'url']\n",
    "test = pd.DataFrame(columns=name, data=list_news)\n",
    "test.to_csv('C:\\Users\\52799\\Desktop\\result.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 进阶加餐-知乎爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "链接如下\n",
    "<br>https://www.zhihu.com/search?q=Datawhale&utm_content=search_history&type=content\n",
    "<br>用requests库实现，不能用selenium网页自动化\n",
    "<br>提示：\n",
    "<br>该链接需要登录，可通过github等，搜索知乎登录的代码实现，并理解其中的逻辑，此任务允许复制粘贴代码\n",
    "<br>与上面ajax加载类似，这次的ajax加载需要用requests完成爬取，最终存储样式随意，但是通过Chrome的开发者工具，分析出ajax的流程需要写出来\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import request\n",
    "from  selenium import webdriver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
